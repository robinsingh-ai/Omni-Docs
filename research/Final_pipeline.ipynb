{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration (Use MPS for Apple M1/M2, fallback to CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model_cache/BAAI_bge-reranker-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def load_models():\n",
    "    \"\"\"Load models for embedding generation and reranking.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"model_cache/BAAI_bge-m3_tokenizer\")\n",
    "    embedding_model = AutoModel.from_pretrained(\"model_cache/BAAI_bge-m3\").to(device)\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "    rerank_model = AutoModelForSequenceClassification.from_pretrained(\"model_cache/BAAI_bge-reranker-large\").to(device)\n",
    "\n",
    "    rerank_tokenizer.save_pretrained(\"model_cache/BAAI_bge-reranker-large\")\n",
    "\n",
    "    return tokenizer, embedding_model, rerank_tokenizer, rerank_model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "\n",
    "\n",
    "tokenizer, embedding_model, rerank_tokenizer, rerank_model = load_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sitemap(sitemap_url: str) -> list:\n",
    "    \"\"\"Fetch URLs from a sitemap.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        sitemap_dict = xmltodict.parse(response.content)\n",
    "        return [url['loc'] for url in sitemap_dict['urlset']['url']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_content(url: str) -> str:\n",
    "    \"\"\"Fetch and clean webpage content.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Clean HTML by removing unnecessary elements\n",
    "        for tag in [\"script\", \"style\", \"meta\", \"noscript\", \"header\", \"footer\", \"nav\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        return \" \".join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Chunk Text for Embedding\n",
    "def chunk_text(text: str, chunk_size: int = 512, overlap: int = 128) -> list:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Embeddings and Sparse Weights (Using Your Provided Code)\n",
    "def generate_embeddings(texts: list, batch_size: int = 32):\n",
    "    \"\"\"Generate embeddings and sparse weights using BGE-M3 and MPS.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Generate embeddings\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=8192, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()  # Get [CLS] token embeddings\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    embeddings_array = np.array(embeddings)\n",
    "    \n",
    "    # Generate sparse weights using token frequencies\n",
    "    sparse_weights = np.zeros((len(texts), tokenizer.vocab_size))\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating sparse weights\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, max_length=8192, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Convert to token frequencies (simple TF scoring)\n",
    "        for j, text_tokens in enumerate(tokens['input_ids'].cpu().numpy()):\n",
    "            unique_tokens, counts = np.unique(text_tokens, return_counts=True)\n",
    "            sparse_weights[i + j][unique_tokens] = counts\n",
    "    \n",
    "    # Normalize sparse weights\n",
    "    sparse_weights = sparse_weights / (sparse_weights.sum(axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    return embeddings_array, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build and Save FAISS Index\n",
    "def build_and_save_index(embeddings: np.ndarray, index_path: str):\n",
    "    \"\"\"Build a FAISS index and save it to a file.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"FAISS index saved to {index_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(scores: np.ndarray):\n",
    "    \"\"\"Normalize the scores to the range [0, 1].\"\"\"\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    \n",
    "    # Avoid division by zero if max_score equals min_score\n",
    "    if max_score != min_score:\n",
    "        return (scores - min_score) / (max_score - min_score)\n",
    "    else:\n",
    "        return scores  # Return as is if scores have no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, index_path: str, sparse_weights: np.ndarray, chunks: list, k: int = 10):\n",
    "    \"\"\"Perform hybrid search with normalized dense and sparse scores.\"\"\"\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Query embedding\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", max_length=8192, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = embedding_model(**inputs).last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    dense_scores, dense_indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Normalize dense scores\n",
    "    dense_scores = normalize_scores(dense_scores[0])\n",
    "\n",
    "    # Sparse scores\n",
    "    query_tokens = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "    sparse_query = np.bincount(query_tokens, minlength=tokenizer.vocab_size)\n",
    "    sparse_query = sparse_query / (sparse_query.sum() + 1e-8)\n",
    "    sparse_scores = np.dot(sparse_weights, sparse_query)\n",
    "\n",
    "    # Normalize sparse scores\n",
    "    sparse_scores = normalize_scores(sparse_scores)\n",
    "\n",
    "    # Combine scores (weighted sum)\n",
    "    dense_weight = 0.7\n",
    "    sparse_weight = 0.3\n",
    "    \n",
    "    combined_scores = {}\n",
    "    for idx, (score, dense_idx) in enumerate(zip(dense_scores, dense_indices[0])):\n",
    "        combined_scores[dense_idx] = dense_weight * score + sparse_weight * sparse_scores[dense_idx]\n",
    "\n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top k results with the corresponding text and URLs\n",
    "    return [(chunks[idx], score, chunk_to_url[idx]) for idx, score in sorted_results[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Rerank Results\n",
    "def rerank_results(query: str, results: list):\n",
    "    \"\"\"Rerank results using the cross-encoder.\"\"\"\n",
    "    pairs = [(query, result[0]) for result in results]\n",
    "    inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = rerank_model(**inputs).logits.squeeze().cpu().numpy()\n",
    "    reranked = sorted([(results[i][0], logits[i]) for i in range(len(results))], key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Cosine Similarity to Check Relevance\n",
    "def compute_similarity(query_embedding: np.ndarray, document_embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute cosine similarity between the query and document embeddings.\"\"\"\n",
    "    return cosine_similarity(query_embedding, document_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query_relevance(query: str, reranked_results: list, threshold: float = 0.5, k: int = 5):\n",
    "    \"\"\"Check the relevance of the query using cosine similarity.\"\"\"\n",
    "    \n",
    "    # Step 1: Generate the embedding for the query\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", max_length=8192, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = embedding_model(**query_inputs).last_hidden_state[:, 0]\n",
    "        query_embedding = query_embedding.detach().cpu().numpy()  # Detach and convert to numpy\n",
    "    \n",
    "    # Step 2: Get the embeddings for the top k documents from reranked_results\n",
    "    top_docs = [result[0] for result in reranked_results[:k]]\n",
    "    top_doc_embeddings = np.array([\n",
    "        embedding_model(**tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True, max_length=8192).to(device)).last_hidden_state[:, 0]\n",
    "        .detach().cpu().numpy()\n",
    "        for doc in top_docs\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Ensure embeddings are 2D for cosine similarity\n",
    "    query_embedding = query_embedding.reshape(1, -1)  # Reshape to (1, hidden_size)\n",
    "    top_doc_embeddings = top_doc_embeddings.reshape(k, -1)  # Reshape to (k, hidden_size)\n",
    "\n",
    "    # Step 4: Compute cosine similarity between the query and the top k documents\n",
    "    similarities = compute_similarity(query_embedding, top_doc_embeddings)\n",
    "\n",
    "    # Step 5: Check the highest similarity score\n",
    "    max_similarity = max(similarities[0])  # Get the highest similarity score from the result\n",
    "    print(f\"Max similarity score: {max_similarity:.4f}\")\n",
    "\n",
    "    if max_similarity < threshold:\n",
    "        print(\"Query is classified as irrelevant.\")\n",
    "        return \"Sorry, your query seems unrelated to the documentation. Please refine your query.\"\n",
    "    else:\n",
    "        print(\"Query is classified as relevant.\")\n",
    "        return reranked_results  # Return the reranked results if relevant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_url = \"https://nextjs.org/sitemap.xml\"\n",
    "urls = fetch_sitemap(sitemap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   1%|          | 6/568 [00:01<01:49,  5.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Processing URLs: 100%|██████████| 568/568 [02:27<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks, chunk_to_url = [], {}\n",
    "for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "    content = fetch_content(url)\n",
    "    if content:\n",
    "        url_chunks = chunk_text(content)\n",
    "        chunk_to_url.update({len(chunks) + i: url for i in range(len(url_chunks))})\n",
    "        chunks.extend(url_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 77/77 [06:39<00:00,  5.19s/it]\n",
      "Generating sparse weights: 100%|██████████| 77/77 [00:01<00:00, 59.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and sparse weights\n",
    "embeddings, sparse_weights = generate_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sparse_weights: (2462, 250002)\n",
      "First few rows of sparse_weights: [[0.00194175 0.00194175 0.00194175 ... 0.         0.         0.        ]\n",
      " [0.00194175 0.         0.00194175 ... 0.         0.         0.        ]\n",
      " [0.00194175 0.00194175 0.00194175 ... 0.         0.         0.        ]\n",
      " [0.00194175 0.25825243 0.00194175 ... 0.         0.         0.        ]\n",
      " [0.00194175 0.00194175 0.00194175 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of sparse_weights: {sparse_weights.shape}\")\n",
    "print(f\"First few rows of sparse_weights: {sparse_weights[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# Convert the sparse_weights to a sparse matrix format\n",
    "sparse_weights_sparse = scipy.sparse.csr_matrix(sparse_weights)\n",
    "\n",
    "# Save the sparse matrix in .npz format\n",
    "scipy.sparse.save_npz(\"sparse_weights.npz\", sparse_weights_sparse)\n",
    "\n",
    "print(\"Sparse weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded successfully!\n",
      "Sparse weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index\n",
    "index_path = \"faiss_index.bin\"\n",
    "index = faiss.read_index(index_path)\n",
    "print(\"FAISS index loaded successfully!\")\n",
    "\n",
    "# Load Sparse Weights from .npz file\n",
    "sparse_weights_loaded = scipy.sparse.load_npz(\"sparse_weights.npz\")\n",
    "print(\"Sparse weights loaded successfully!\")\n",
    "\n",
    "# Convert sparse matrix to dense numpy array if needed for compatibility\n",
    "# If you want to use sparse format, make sure your hybrid_search function is compatible with sparse matrices\n",
    "sparse_weights_dense = sparse_weights_loaded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max similarity score: 0.5519\n",
      "Query is classified as relevant.\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Score: -0.0056\n",
      "Text: ly appreciate your feedback. If you are experiencing any issues with Next.js, please open an issue , or start a new discussion , and we will investigate....\n",
      "\n",
      "Score: -0.0166\n",
      "Text: , and other external system synchronization. Since these tasks are not directly related to the response, the user should not have to wait for them to complete. Deferring the work after responding to t...\n",
      "\n",
      "Score: -0.0666\n",
      "Text: green for performant applications. The following options are available for the next build command: Option Description -h, --help Show all available options. [directory] A directory on which to build t...\n",
      "\n",
      "Score: -0.0666\n",
      "Text: green for performant applications. The following options are available for the next build command: Option Description -h, --help Show all available options. [directory] A directory on which to build t...\n",
      "\n",
      "Score: -0.0846\n",
      "Text: to do next, we recommend the following sections Linking and Navigating Learn how navigation works in Next.js, and how to use the Link Component and `useRouter` hook. useRouter Learn more about the API...\n",
      "\n",
      "Score: -0.1511\n",
      "Text: CLI: next CLI | Next.js Menu Using App Router Features available in /app Using Latest Version 15.1.6 API Reference CLI next CLI next CLI The Next.js CLI allows you to develop, build, start your applic...\n",
      "\n",
      "Score: -0.1511\n",
      "Text: CLI: next CLI | Next.js Menu Using App Router Features available in /app Using Latest Version 15.1.6 API Reference CLI next CLI next CLI The Next.js CLI allows you to develop, build, start your applic...\n",
      "\n",
      "Score: -0.1511\n",
      "Text: Next.js application. Was this helpful? supported. Send...\n",
      "\n",
      "Score: -0.1529\n",
      "Text: struggle to decide where to place them in an application for optimal loading. With next/script , you can define the strategy property and Next.js will automatically prioritize them to improve loading ...\n",
      "\n",
      "Score: -0.1598\n",
      "Text: next' const config : NextConfig = { images : { remotePatterns : [ { protocol : 'https' , hostname : 's3.amazonaws.com' , port : '' , pathname : '/my-bucket/**' , search : '' , } , ] , } , } export def...\n"
     ]
    }
   ],
   "source": [
    "# Now you can use these loaded values in your pipeline\n",
    "query = \"What is next\"  # An irrelevant query\n",
    "\n",
    "# Call the hybrid_search function with the loaded FAISS index and sparse weights\n",
    "results = hybrid_search(query, index_path, sparse_weights_dense, chunks)\n",
    "\n",
    "# Perform reranking\n",
    "reranked_results = rerank_results(query, results)\n",
    "\n",
    "# Check relevance based on similarity\n",
    "final_results = check_query_relevance(query, reranked_results)\n",
    "\n",
    "# Display results or fallback message\n",
    "if isinstance(final_results, str):\n",
    "    print(final_results)  # Print fallback message if irrelevant\n",
    "else:\n",
    "    print(\"\\nSearch Results:\")\n",
    "    for text, score in final_results:\n",
    "        print(f\"\\nScore: {score:.4f}\")\n",
    "        print(f\"Text: {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextjs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
