{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "from fastembed import TextEmbedding, SparseTextEmbedding\n",
    "from fastembed.rerank.cross_encoder import TextCrossEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class RetrievalPipeline:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "        self.device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize models (removed CoreML provider for stability)\n",
    "        self.dense_model = TextEmbedding(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "        )\n",
    "        \n",
    "        self.sparse_model = SparseTextEmbedding(\n",
    "            model_name=\"prithivida/Splade_PP_en_v1\"\n",
    "        )\n",
    "        \n",
    "        self.reranker = TextCrossEncoder(\n",
    "            model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\"\n",
    "        )\n",
    "        \n",
    "        # Data storage\n",
    "        self.chunks: List[str] = []\n",
    "        self.chunk_to_url: Dict[str, str] = {}\n",
    "        self.sparse_embeddings = []\n",
    "        self.dense_embeddings: Optional[torch.Tensor] = None\n",
    "        self.dense_index: Optional[faiss.Index] = None\n",
    "\n",
    "    def fetch_sitemap(self, sitemap_url: str) -> List[str]:\n",
    "        \"\"\"Fetch URLs from sitemap with error handling.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(sitemap_url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            sitemap_dict = xmltodict.parse(response.content)\n",
    "            return [url['loc'] for url in sitemap_dict['urlset']['url']]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching sitemap: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_content(self, url: str) -> str:\n",
    "        \"\"\"Fetch and clean content from URL with improved error handling.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"form\"]):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Clean text\n",
    "            text = ' '.join(\n",
    "                line.strip() \n",
    "                for line in soup.get_text().splitlines() \n",
    "                if line.strip() and len(line.strip()) > 25\n",
    "            )\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 128) -> List[str]:\n",
    "        \"\"\"Token-based chunking with overlap.\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            end = start + chunk_size\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunks.append(' '.join(chunk_tokens))\n",
    "            start = end - overlap if end - overlap > start else start + 1\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def normalize_scores(self, scores: List[float]) -> List[float]:\n",
    "        \"\"\"Robust score normalization handling multiple input types.\"\"\"\n",
    "        if not scores:\n",
    "            return []\n",
    "        \n",
    "        # Convert generators to numpy arrays\n",
    "        if isinstance(scores, (map, filter, Generator)):\n",
    "            scores = list(scores)\n",
    "        \n",
    "        scores = np.array(scores, dtype=np.float32)\n",
    "        \n",
    "        if scores.size == 0:\n",
    "            return []\n",
    "        \n",
    "        if np.ptp(scores) == 0:\n",
    "            return np.ones_like(scores).tolist()\n",
    "        \n",
    "        return ((scores - np.min(scores)) / np.ptp(scores)).tolist()\n",
    "\n",
    "    def process_documents(self, sitemap_url: str):\n",
    "        \"\"\"Process documents with type-safe conversions and device handling\"\"\"\n",
    "        data_dir = Path(\"data\")\n",
    "        data_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        urls = self.fetch_sitemap(sitemap_url)\n",
    "        print(f\"Found {len(urls)} URLs\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        chunk_to_url = {}\n",
    "        \n",
    "        # Process URLs and chunk content\n",
    "        for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "            content = self.fetch_content(url)\n",
    "            if not content:\n",
    "                continue\n",
    "                \n",
    "            chunks = self.chunk_text(content)\n",
    "            for chunk in chunks:\n",
    "                chunk_idx = str(len(all_chunks))\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_to_url[chunk_idx] = url\n",
    "        \n",
    "        # Generate dense embeddings with explicit type handling\n",
    "        print(\"Generating dense embeddings...\")\n",
    "        embeddings = list(self.dense_model.embed(all_chunks))\n",
    "        \n",
    "        # Create numpy array first with proper dtype\n",
    "        dense_numpy = np.stack(embeddings).astype(np.float32)\n",
    "        \n",
    "        # Convert directly to tensor with device placement\n",
    "        self.dense_embeddings = torch.tensor(\n",
    "            dense_numpy,\n",
    "            device=self.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Build FAISS index using CPU numpy array\n",
    "        print(\"Building FAISS index...\")\n",
    "        self.dense_index = faiss.IndexFlatIP(dense_numpy.shape[1])\n",
    "        self.dense_index.add(dense_numpy.astype('float32'))  # Extra type safety\n",
    "        \n",
    "        # Generate sparse embeddings\n",
    "        print(\"Generating sparse embeddings...\")\n",
    "        self.sparse_embeddings = list(self.sparse_model.embed(all_chunks))\n",
    "        \n",
    "        # Save all components\n",
    "        print(\"Saving data...\")\n",
    "        with open(data_dir/'chunks.pkl', 'wb') as f:\n",
    "            pickle.dump(all_chunks, f)\n",
    "        with open(data_dir/'chunk_to_url.json', 'w') as f:\n",
    "            json.dump(chunk_to_url, f)\n",
    "        with open(data_dir/'sparse_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(self.sparse_embeddings, f)\n",
    "        faiss.write_index(self.dense_index, str(data_dir/'dense_index.faiss'))\n",
    "        torch.save(self.dense_embeddings, data_dir/'dense_embeddings.pt')\n",
    "        \n",
    "        print(f\"Processed {len(all_chunks)} chunks successfully\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load processed data with enhanced error handling.\"\"\"\n",
    "        data_dir = Path(\"data\")\n",
    "        required_files = [\n",
    "            'chunks.pkl', \n",
    "            'chunk_to_url.json',\n",
    "            'sparse_embeddings.pkl',\n",
    "            'dense_index.faiss',\n",
    "            'dense_embeddings.pt'\n",
    "        ]\n",
    "        \n",
    "        for file in required_files:\n",
    "            if not (data_dir/file).exists():\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "        \n",
    "        with open(data_dir/'chunks.pkl', 'rb') as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        with open(data_dir/'chunk_to_url.json', 'r') as f:\n",
    "            self.chunk_to_url = json.load(f)\n",
    "        with open(data_dir/'sparse_embeddings.pkl', 'rb') as f:\n",
    "            self.sparse_embeddings = pickle.load(f)\n",
    "        self.dense_index = faiss.read_index(str(data_dir/'dense_index.faiss'))\n",
    "        self.dense_embeddings = torch.load(data_dir/'dense_embeddings.pt', map_location=self.device)\n",
    "\n",
    "    def hybrid_search(self, query: str, k: int = 50) -> List[Tuple[int, str, float, float, float]]:\n",
    "        \"\"\"Optimized hybrid search with combined scoring.\"\"\"\n",
    "        # Dense search\n",
    "        query_dense = np.array(list(self.dense_model.embed([query]))[0])\n",
    "        dense_scores, dense_indices = self.dense_index.search(query_dense.reshape(1, -1), k)\n",
    "        dense_scores = dense_scores[0].tolist()\n",
    "        top_indices = dense_indices[0].tolist()\n",
    "        \n",
    "        # Sparse scoring only on top results\n",
    "        query_sparse = list(self.sparse_model.embed([query]))[0]\n",
    "        sparse_scores = []\n",
    "        query_indices = set(query_sparse.indices)\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            doc_emb = self.sparse_embeddings[idx]\n",
    "            score = sum(\n",
    "                query_sparse.values[i] * doc_emb.values[doc_emb.indices == idx][0]\n",
    "                for i, idx in enumerate(query_sparse.indices)\n",
    "                if idx in doc_emb.indices\n",
    "            )\n",
    "            sparse_scores.append(score)\n",
    "        \n",
    "        # Normalize and combine scores\n",
    "        norm_dense = self.normalize_scores(dense_scores)\n",
    "        norm_sparse = self.normalize_scores(sparse_scores)\n",
    "        \n",
    "        combined = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            combined_score = 0.7 * norm_dense[i] + 0.3 * norm_sparse[i]\n",
    "            combined.append((idx, combined_score, norm_dense[i], norm_sparse[i]))\n",
    "        \n",
    "        # Sort by combined score\n",
    "        combined.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [(idx, self.chunks[idx], *scores) for idx, *scores in combined[:k]]\n",
    "\n",
    "    def check_relevance(self, query: str, results: List[Tuple[int, str, float, float, float]], \n",
    "                       threshold: float = 0.6) -> Tuple[bool, float]:\n",
    "        \"\"\"Enhanced relevance check with term normalization\"\"\"\n",
    "        if not results:\n",
    "            return False, 0.0\n",
    "            \n",
    "        # Get top result\n",
    "        top_idx = results[0][0]\n",
    "        top_text = results[0][1]\n",
    "        \n",
    "        # Semantic similarity\n",
    "        query_embedding = torch.tensor(list(self.dense_model.embed([query]))[0], device=self.device)\n",
    "        doc_embedding = self.dense_embeddings[top_idx]\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            query_embedding, doc_embedding, dim=0\n",
    "        ).item()\n",
    "        \n",
    "        # Term normalization\n",
    "        def normalize_text(text: str) -> set:\n",
    "            # Remove punctuation and stopwords\n",
    "            text = ''.join([c.lower() if c.isalnum() else ' ' for c in text])\n",
    "            return set(word for word in text.split() if word not in self.stop_words)\n",
    "        \n",
    "        query_terms = normalize_text(query)\n",
    "        doc_terms = normalize_text(top_text)\n",
    "        \n",
    "        # Flexible term matching\n",
    "        term_overlap = len(query_terms & doc_terms) / len(query_terms) if query_terms else 0\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"\\nRelevance Check:\")\n",
    "        print(f\"Top Document: {top_text[:200]}...\")\n",
    "        print(f\"Similarity: {similarity:.2f}, Term Overlap: {term_overlap:.2f}\")\n",
    "        \n",
    "        # Adjusted thresholds\n",
    "        return (\n",
    "            similarity >= threshold and term_overlap >= 0.25,\n",
    "            similarity\n",
    "        )\n",
    "\n",
    "    def search(self, query: str) -> Dict:\n",
    "        \"\"\"Fixed search with proper score handling\"\"\"\n",
    "        results = self.hybrid_search(query)\n",
    "        \n",
    "        # Early exit if no relevant results\n",
    "        is_relevant, rel_score = self.check_relevance(query, results)\n",
    "        if not is_relevant:\n",
    "            return {\n",
    "                \"status\": \"no_results\",\n",
    "                \"message\": \"No relevant documents found\",\n",
    "                \"confidence\": f\"{rel_score:.2f}\"\n",
    "            }\n",
    "        \n",
    "        # Rerank top candidates with type conversion\n",
    "        candidates = [res[1] for res in results[:20]]\n",
    "        rerank_scores = list(self.reranker.rerank(query, candidates))  # Convert generator to list\n",
    "        rerank_scores = self.normalize_scores(rerank_scores)\n",
    "        \n",
    "        # Combine scores\n",
    "        final_results = []\n",
    "        for (idx, text, comb, dense, sparse), rerank in zip(results[:10], rerank_scores[:10]):\n",
    "            final_score = 0.5 * comb + 0.5 * rerank\n",
    "            final_results.append({\n",
    "                \"index\": idx,\n",
    "                \"text\": text[:350] + \"...\" if len(text) > 350 else text,\n",
    "                \"url\": self.chunk_to_url.get(str(idx), \"\"),\n",
    "                \"scores\": {\n",
    "                    \"final\": round(final_score, 3),\n",
    "                    \"dense\": round(dense, 3),\n",
    "                    \"sparse\": round(sparse, 3),\n",
    "                    \"rerank\": round(rerank, 3)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"confidence\": f\"{rel_score:.2f}\",\n",
    "            \"results\": sorted(final_results, key=lambda x: x['scores']['final'], reverse=True)\n",
    "        }\n",
    "\n",
    "    def print_results(self, search_results: Dict):\n",
    "        \"\"\"Enhanced result formatting.\"\"\"\n",
    "        if search_results[\"status\"] != \"success\":\n",
    "            print(f\"\\n❌ {search_results['message']} (Confidence: {search_results['confidence']})\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\n🔍 Found {len(search_results['results'])} results (Confidence: {search_results['confidence']})\")\n",
    "        for result in search_results[\"results\"]:\n",
    "            print(f\"\\n📄 {result['url']}\")\n",
    "            print(f\"📝 {result['text']}\")\n",
    "            print(\"📊 Scores:\")\n",
    "            print(f\"  Final: {result['scores']['final']}\")\n",
    "            print(f\"  Dense: {result['scores']['dense']}\")\n",
    "            print(f\"  Sparse: {result['scores']['sparse']}\")\n",
    "            print(f\"  Rerank: {result['scores']['rerank']}\")\n",
    "            print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "pipeline = RetrievalPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 570 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 570/570 [02:15<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dense embeddings...\n",
      "Building FAISS index...\n",
      "Generating sparse embeddings...\n",
      "Saving data...\n",
      "Processed 1574 chunks successfully\n"
     ]
    }
   ],
   "source": [
    "pipeline.process_documents(\"https://nextjs.org/sitemap.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relevance Check:\n",
      "Top Document: Rendering : Server-side Rendering ( SSR ) | Next.js MenuUsing App RouterFeatures available in /appUsing Latest Version15.1.6Building Your ApplicationRenderingServer-side Rendering ( SSR ) Server-side ...\n",
      "Similarity: 0.70, Term Overlap: 1.00\n"
     ]
    }
   ],
   "source": [
    "results = pipeline.search(\"SSR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Found 10 results (Confidence: 0.70)\n",
      "\n",
      "📄 https://nextjs.org/docs/pages/building-your-application/rendering/server-side-rendering\n",
      "📝 Rendering : Server-side Rendering ( SSR ) | Next.js MenuUsing App RouterFeatures available in /appUsing Latest Version15.1.6Building Your ApplicationRenderingServer-side Rendering ( SSR ) Server-side Rendering ( SSR ) Also referred to as `` SSR '' or `` Dynamic Rendering '' . If a page uses Server-side Rendering , the page HTML is generated on each...\n",
      "📊 Scores:\n",
      "  Final: 1.0\n",
      "  Dense: 1.0\n",
      "  Sparse: 1.0\n",
      "  Rerank: 1.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/learn/seo/rendering-strategies\n",
      "📝 SEO : Rendering Strategies | Next.jsSign inSign in to save progress11Chapter 11Rendering StrategiesStatic Site Generation ( SSG ) Static site generation is where your HTML is generated at build time . This HTML is then used for each request . Static site generation is probably the best type of rendering strategy for SEO as not only do you have all ...\n",
      "📊 Scores:\n",
      "  Final: 0.61\n",
      "  Dense: 0.568\n",
      "  Sparse: 0.74\n",
      "  Rerank: 0.6\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/docs/app/building-your-application/optimizing/lazy-loading\n",
      "📝 pre-rendering for a Client Component , you can use the ssr option set to false : const ComponentC = dynamic ( ( ) = > import ( ' .. /components/C ' ) , { ssr : false } ) Importing Server Components If you dynamically import a Server Component , only the Client Components that are children of the Server Component will be lazy-loaded - not the Server...\n",
      "📊 Scores:\n",
      "  Final: 0.598\n",
      "  Dense: 0.341\n",
      "  Sparse: 0.828\n",
      "  Rerank: 0.708\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/blog/next-9-3\n",
      "📝 unified way to do data fetching and static generation . Today we 're incredibly excited to announce two new data fetching methods : getStaticProps and getServerSideProps . We also include a way to provide parameters to statically generate static pages for dynamic routes : getStaticPaths . These new methods have many advantages over the getInitialPr...\n",
      "📊 Scores:\n",
      "  Final: 0.465\n",
      "  Dense: 0.253\n",
      "  Sparse: 0.712\n",
      "  Rerank: 0.539\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/docs/pages/api-reference/functions/use-router\n",
      "📝 url that will be shown in the browser options : Object - Additional options sent by router.push If cb returns false , the Next.js router will not handle popstate , and you 'll be responsible for handling it in that case . See Disabling file-system routing . You could use beforePopState to manipulate the request , or force a SSR refresh , as in the ...\n",
      "📊 Scores:\n",
      "  Final: 0.452\n",
      "  Dense: 0.209\n",
      "  Sparse: 0.589\n",
      "  Rerank: 0.581\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/blog/next-9-5\n",
      "📝 attempt to re-generate the page : // - when a request comes in // - at most once every second The revalidate flag is the number of seconds during which at most one generation will happen , to prevent a https : //en.wikipedia.org/wiki/Cache_stampede . Unlike traditional SSR , Incremental Static Regeneration ensures you retain the benefits of static ...\n",
      "📊 Scores:\n",
      "  Final: 0.374\n",
      "  Dense: 0.018\n",
      "  Sparse: 0.749\n",
      "  Rerank: 0.511\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/docs/app/building-your-application/upgrading/single-page-applications\n",
      "📝 SWRSSR dataStreaming while SSRDeduplicate requestsClient-side features You can use React Query with Next.js on both the client and server . This enables you to build both strict SPAs , as well as take advantage of server features in Next.js paired with React Query . Learn more in the React Query documentation . Rendering components only in the brow...\n",
      "📊 Scores:\n",
      "  Final: 0.364\n",
      "  Dense: 0.149\n",
      "  Sparse: 0.473\n",
      "  Rerank: 0.482\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/learn/seo/url-structure\n",
      "📝 products or blogs subfolder . Here 's an example of a page optimized for this using SSG : import Head from 'next/head ' ; export async function getStaticPaths ( ) { // Call an external API endpoint to get posts const res = await fetch ( 'https : //www.example.com/api/posts ' ) ; const posts = await res.json ( ) ; // Get the paths we want to pre-ren...\n",
      "📊 Scores:\n",
      "  Final: 0.333\n",
      "  Dense: 0.298\n",
      "  Sparse: 0.0\n",
      "  Rerank: 0.458\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/learn/dashboard-app/fetching-data\n",
      "📝 className= '' grid gap-6 sm : grid-cols-2 lg : grid-cols-4 '' > { / * < Card title= '' Collected '' value= { totalPaidInvoices } type= '' collected '' / > * / } { / * < Card title= '' Pending '' value= { totalPendingInvoices } type= '' pending '' / > * / } { / * < Card title= '' Total Invoices '' value= { numberOfInvoices } type= '' invoices '' / >...\n",
      "📊 Scores:\n",
      "  Final: 0.135\n",
      "  Dense: 0.381\n",
      "  Sparse: 0.003\n",
      "  Rerank: 0.003\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 https://nextjs.org/blog/next-9-4\n",
      "📝 track the improvement or regression of these metrics over time in order to see if your changes have the intended impact on your audience . In order to aid reporting Core Web Vitals to your analytics service we have introduced , in collaboration with Google , a new method called reportWebVitals which can be exported from pages/_app.js : pages/_app.j...\n",
      "📊 Scores:\n",
      "  Final: 0.129\n",
      "  Dense: 0.312\n",
      "  Sparse: 0.0\n",
      "  Rerank: 0.04\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pipeline.print_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!conda list -e > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextjs_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
