{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers langchain chromadb scikit-learn nltk datasets evaluate beautifulsoup4 requests lxml faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robinsingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/robinsingh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Directory structure\n",
    "        self.models_dir = \"models\"\n",
    "        self.indexes_dir = \"indexes\"\n",
    "        self.cache_dir = \"cache\"\n",
    "        self.raw_data_dir = \"raw_data\"\n",
    "        self.processed_dir = \"processed_data\"\n",
    "        \n",
    "        # Model parameters\n",
    "        self.max_tokens = 512\n",
    "        self.top_k = 5\n",
    "        \n",
    "        # Create necessary directories\n",
    "        for dir_path in [self.models_dir, self.indexes_dir, self.cache_dir, \n",
    "                        self.raw_data_dir, self.processed_dir]:\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Content Fetching and Processing\n",
    "class ContentFetcher:\n",
    "    def __init__(self, sitemap_url):\n",
    "        self.sitemap_url = sitemap_url\n",
    "    \n",
    "    def fetch_urls(self, limit=10):\n",
    "        print(f\"Fetching up to {limit} URLs from sitemap...\")\n",
    "        response = requests.get(self.sitemap_url)\n",
    "        root = ET.fromstring(response.content)\n",
    "        urls = []\n",
    "        \n",
    "        for url in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
    "            if len(urls) < limit:\n",
    "                urls.append(url.text)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(urls)} URLs\")\n",
    "        return urls\n",
    "    \n",
    "    def fetch_and_save_content(self, urls):\n",
    "        print(\"Fetching content from URLs...\")\n",
    "        contents = {}\n",
    "        \n",
    "        for url in tqdm(urls):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Remove script and style elements\n",
    "                for element in soup([\"script\", \"style\"]):\n",
    "                    element.decompose()\n",
    "                \n",
    "                # Get text content\n",
    "                text = soup.get_text()\n",
    "                \n",
    "                # Basic cleaning\n",
    "                text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                text = ' '.join(text.split())\n",
    "                \n",
    "                contents[url] = text\n",
    "                \n",
    "                # Save raw content\n",
    "                filename = url.split('/')[-1] or 'index'\n",
    "                filename = f\"{filename}.txt\"\n",
    "                with open(os.path.join(config.raw_data_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {str(e)}\")\n",
    "        \n",
    "        # Save URL to content mapping\n",
    "        with open(os.path.join(config.raw_data_dir, 'url_mapping.json'), 'w') as f:\n",
    "            json.dump(contents, f, indent=2)\n",
    "        \n",
    "        return contents\n",
    "    \n",
    "    def chunk_and_save_text(self, contents):\n",
    "        print(\"Chunking text into smaller segments...\")\n",
    "        all_chunks = []\n",
    "        chunk_mapping = {}\n",
    "        \n",
    "        for url, text in contents.items():\n",
    "            sentences = sent_tokenize(text)\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_length = len(sentence.split())\n",
    "                if current_length + sentence_length > config.max_tokens:\n",
    "                    if current_chunk:\n",
    "                        chunk_text = \" \".join(current_chunk)\n",
    "                        chunks.append(chunk_text)\n",
    "                        all_chunks.append(chunk_text)\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "                    current_length += sentence_length\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunks.append(chunk_text)\n",
    "                all_chunks.append(chunk_text)\n",
    "            \n",
    "            chunk_mapping[url] = chunks\n",
    "        \n",
    "        # Save all chunks and mapping\n",
    "        with open(os.path.join(config.processed_dir, 'all_chunks.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_chunks, f, indent=2)\n",
    "        \n",
    "        with open(os.path.join(config.processed_dir, 'chunk_mapping.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunk_mapping, f, indent=2)\n",
    "        \n",
    "        return all_chunks, chunk_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model Management\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.model_configs = {\n",
    "            'e5': 'intfloat/e5-base-v2',\n",
    "            'sbert': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "        }\n",
    "    \n",
    "    def download_and_save_models(self):\n",
    "        print(\"Downloading and saving models...\")\n",
    "        for model_name, model_path in self.model_configs.items():\n",
    "            print(f\"\\nProcessing {model_name} from {model_path}...\")\n",
    "            model_save_path = os.path.join(config.models_dir, model_name)\n",
    "            \n",
    "            if not os.path.exists(model_save_path):\n",
    "                if model_name == 'sbert':\n",
    "                    model = SentenceTransformer(model_path)\n",
    "                    model.save(model_save_path)\n",
    "                else:\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "                    model = AutoModel.from_pretrained(model_path)\n",
    "                    \n",
    "                    tokenizer.save_pretrained(model_save_path)\n",
    "                    model.save_pretrained(model_save_path)\n",
    "                print(f\"Successfully downloaded and saved {model_name}\")\n",
    "            else:\n",
    "                print(f\"{model_name} already exists in local storage\")\n",
    "    \n",
    "    def load_models(self):\n",
    "        print(\"Loading models...\")\n",
    "        for model_name in self.model_configs.keys():\n",
    "            model_path = os.path.join(config.models_dir, model_name)\n",
    "            if not os.path.exists(model_path):\n",
    "                raise ValueError(f\"Model {model_name} not found. Run download_and_save_models first.\")\n",
    "            \n",
    "            if model_name == 'sbert':\n",
    "                self.models[model_name] = SentenceTransformer(model_path)\n",
    "            else:\n",
    "                self.tokenizers[model_name] = AutoTokenizer.from_pretrained(model_path)\n",
    "                self.models[model_name] = AutoModel.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Indexing and Vectorization\n",
    "class VectorIndexer:\n",
    "    def __init__(self, model_manager):\n",
    "        self.model_manager = model_manager\n",
    "        self.indexes = {}\n",
    "    \n",
    "    def generate_embeddings(self, text, model_name):\n",
    "        if model_name == 'sbert':\n",
    "            return self.model_manager.models[model_name].encode([text])[0]\n",
    "        else:\n",
    "            model = self.model_manager.models[model_name]\n",
    "            tokenizer = self.model_manager.tokenizers[model_name]\n",
    "            \n",
    "            inputs = tokenizer(text, padding=True, truncation=True, \n",
    "                             return_tensors=\"pt\", max_length=config.max_tokens)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).numpy()[0]\n",
    "    \n",
    "    def create_indexes(self, chunks):\n",
    "        print(\"Creating indexes...\")\n",
    "        # Create TF-IDF index\n",
    "        print(\"\\nCreating TF-IDF index...\")\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf_vectors = tfidf.fit_transform(chunks)\n",
    "        \n",
    "        with open(os.path.join(config.indexes_dir, 'tfidf.pkl'), 'wb') as f:\n",
    "            pickle.dump((tfidf, tfidf_vectors), f)\n",
    "        \n",
    "        # Create FAISS indexes\n",
    "        for model_name in self.model_manager.model_configs.keys():\n",
    "            print(f\"\\nCreating FAISS index for {model_name}...\")\n",
    "            embeddings = []\n",
    "            \n",
    "            for chunk in tqdm(chunks):\n",
    "                embedding = self.generate_embeddings(chunk, model_name)\n",
    "                embeddings.append(embedding)\n",
    "            \n",
    "            embeddings = np.array(embeddings)\n",
    "            index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            index.add(embeddings)\n",
    "            \n",
    "            faiss.write_index(index, os.path.join(config.indexes_dir, f\"{model_name}.index\"))\n",
    "            print(f\"Index saved for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main Pipeline Class\n",
    "class DocumentRetrieval:\n",
    "    def __init__(self):\n",
    "        self.content_fetcher = None\n",
    "        self.model_manager = ModelManager()\n",
    "        self.vector_indexer = None\n",
    "        self.chunks = None\n",
    "        self.tfidf = None\n",
    "        self.tfidf_vectors = None\n",
    "        self.indexes = {}\n",
    "    \n",
    "    def setup(self, sitemap_url):\n",
    "        print(\"\\nStep 1: Fetching and processing content...\")\n",
    "        self.content_fetcher = ContentFetcher(sitemap_url)\n",
    "        urls = self.content_fetcher.fetch_urls()\n",
    "        contents = self.content_fetcher.fetch_and_save_content(urls)\n",
    "        self.chunks, _ = self.content_fetcher.chunk_and_save_text(contents)\n",
    "        \n",
    "        print(\"\\nStep 2: Downloading and loading models...\")\n",
    "        self.model_manager.download_and_save_models()\n",
    "        self.model_manager.load_models()\n",
    "        \n",
    "        print(\"\\nStep 3: Creating indexes...\")\n",
    "        self.vector_indexer = VectorIndexer(self.model_manager)\n",
    "        self.vector_indexer.create_indexes(self.chunks)\n",
    "        \n",
    "        print(\"\\nSetup complete!\")\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        # Load TF-IDF\n",
    "        with open(os.path.join(config.indexes_dir, 'tfidf.pkl'), 'rb') as f:\n",
    "            self.tfidf, self.tfidf_vectors = pickle.load(f)\n",
    "        \n",
    "        # Load FAISS indexes\n",
    "        for model_name in self.model_manager.model_configs.keys():\n",
    "            index_path = os.path.join(config.indexes_dir, f\"{model_name}.index\")\n",
    "            self.indexes[model_name] = faiss.read_index(index_path)\n",
    "    \n",
    "    def query(self, query_text, k=5):\n",
    "        if not self.indexes:\n",
    "            self.load_indexes()\n",
    "        \n",
    "        # TF-IDF search\n",
    "        query_vector = self.tfidf.transform([query_text])\n",
    "        tfidf_scores = (query_vector * self.tfidf_vectors.T).toarray()[0]\n",
    "        keyword_indices = np.argsort(tfidf_scores)[-k:][::-1]\n",
    "        \n",
    "        # Semantic search\n",
    "        semantic_results = {}\n",
    "        for model_name, index in self.indexes.items():\n",
    "            query_embedding = self.vector_indexer.generate_embeddings(query_text, model_name)\n",
    "            D, I = index.search(query_embedding.reshape(1, -1), k)\n",
    "            semantic_results[model_name] = {'indices': I[0], 'distances': D[0]}\n",
    "        \n",
    "        # Combine results\n",
    "        candidate_indices = set(keyword_indices)\n",
    "        for results in semantic_results.values():\n",
    "            candidate_indices.update(results['indices'])\n",
    "        \n",
    "        # Get final documents and scores\n",
    "        final_docs = [self.chunks[i] for i in candidate_indices]\n",
    "        final_scores = [tfidf_scores[i] for i in candidate_indices]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_pairs = sorted(zip(final_docs, final_scores), key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in sorted_pairs[:k]], [score for _, score in sorted_pairs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Fetching and processing content...\n",
      "Fetching up to 10 URLs from sitemap...\n",
      "Found 10 URLs\n",
      "Fetching content from URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text into smaller segments...\n",
      "\n",
      "Step 2: Downloading and loading models...\n",
      "Downloading and saving models...\n",
      "\n",
      "Processing e5 from intfloat/e5-base-v2...\n",
      "Successfully downloaded and saved e5\n",
      "\n",
      "Processing sbert from sentence-transformers/all-MiniLM-L6-v2...\n",
      "Successfully downloaded and saved sbert\n",
      "Loading models...\n",
      "\n",
      "Step 3: Creating indexes...\n",
      "Creating indexes...\n",
      "\n",
      "Creating TF-IDF index...\n",
      "\n",
      "Creating FAISS index for e5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:07<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved for e5\n",
      "\n",
      "Creating FAISS index for sbert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 42.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved for sbert\n",
      "\n",
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Usage Example\n",
    "# Initialize the pipeline\n",
    "retrieval = DocumentRetrieval()\n",
    "\n",
    "# Run setup (only need to do this once)\n",
    "retrieval.setup(\"https://nextjs.org/sitemap.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can run multiple queries\n",
    "def search_docs(query):\n",
    "    results, scores = retrieval.query(query)\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\\nTop 5 Results:\")\n",
    "    for i, (doc, score) in enumerate(zip(results, scores), 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"Document: {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: apple silicon support in nextjs\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "1. Score: 0.1940\n",
      "Document: Next.js 10.1 | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnBack to BlogMonday, March 29th 2021Next.js 10.1Po...\n",
      "\n",
      "2. Score: 0.0942\n",
      "Document: When the nextjs.org website launched, we would manually keep the documentation changes up to date by periodically copying the content from the Next.js GitHub repository to the website GitHub repositor...\n",
      "\n",
      "3. Score: 0.0930\n",
      "Document: Next.js, the React framework for production, enables you to incrementally adopt React.Read MoreOctober 27th, 2020+5Next.js 10We are excited to introduce Next.js 10, featuring: Built-in Image Component...\n",
      "\n",
      "4. Score: 0.0574\n",
      "Document: Additional Layouts Based on your feedback, we've added a variety of new layouts and options for next/image: layout=fill: You don't need to provide width and height. (Demo) layout=fixed: Native img beh...\n",
      "\n",
      "5. Score: 0.0507\n",
      "Document: New Next.js Documentation | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnBack to BlogThursday, January 9th 20...\n"
     ]
    }
   ],
   "source": [
    "search_docs(\"apple silicon support in nextjs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what is nextjs\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "1. Score: 0.1533\n",
      "Document: When the nextjs.org website launched, we would manually keep the documentation changes up to date by periodically copying the content from the Next.js GitHub repository to the website GitHub repositor...\n",
      "\n",
      "2. Score: 0.0926\n",
      "Document: New Next.js Documentation | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnBack to BlogThursday, January 9th 20...\n",
      "\n",
      "3. Score: 0.0639\n",
      "Document: This is sometimes called the “donut” pattern: The outer part of the donut is a server component that handles data fetching or heavy logic. The hole in the middle is a child component that might have s...\n",
      "\n",
      "4. Score: 0.0574\n",
      "Document: Sites that load personalized content or ads may also experience wildly different performance from user to user. An emulated test cannot capture these important signals. Next.js Speed Insights allows y...\n",
      "\n",
      "5. Score: 0.0499\n",
      "Document: Some benefits of server-centric routing with React Server Components include: Routing uses the same request used for Server Components (no additional server requests are made). Less work is done on th...\n"
     ]
    }
   ],
   "source": [
    "search_docs(\"what is nextjs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, retrieval_system):\n",
    "        self.retrieval = retrieval_system\n",
    "        \n",
    "    def evaluate_single_model(self, model_name, query, k=5):\n",
    "        \"\"\"Evaluate a single model's performance on a query\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.retrieval.vector_indexer.generate_embeddings(query, model_name)\n",
    "        \n",
    "        # Search using FAISS\n",
    "        D, I = self.retrieval.indexes[model_name].search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Get the documents\n",
    "        results = [self.retrieval.chunks[i] for i in I[0]]\n",
    "        scores = [-d for d in D[0]]  # Convert distances to scores (negative distance)\n",
    "        \n",
    "        return results, scores, response_time\n",
    "    \n",
    "    def evaluate_models(self, test_queries):\n",
    "        \"\"\"\n",
    "        Evaluate each model separately\n",
    "        \"\"\"\n",
    "        models = ['e5', 'sbert']\n",
    "        results = {model: {\n",
    "            \"response_times\": [],\n",
    "            \"keyword_matches\": [],\n",
    "            \"ndcg_scores\": []\n",
    "        } for model in models}\n",
    "        \n",
    "        print(\"\\nStarting Model-by-Model Evaluation:\")\n",
    "        for model in models:\n",
    "            print(f\"\\n{'-'*20}\")\n",
    "            print(f\"Evaluating {model.upper()} Model\")\n",
    "            print(f\"{'-'*20}\")\n",
    "            \n",
    "            for test_case in test_queries:\n",
    "                query = test_case[\"query\"]\n",
    "                expected = test_case[\"expected_keywords\"]\n",
    "                relevance = test_case[\"relevance_scores\"]\n",
    "                \n",
    "                # Get results for this model\n",
    "                docs, scores, response_time = self.evaluate_single_model(model, query)\n",
    "                \n",
    "                # Store response time\n",
    "                results[model][\"response_times\"].append(response_time)\n",
    "                \n",
    "                # Check keyword matches\n",
    "                keyword_matches = []\n",
    "                for doc in docs:\n",
    "                    matches = sum(1 for keyword in expected if keyword.lower() in doc.lower())\n",
    "                    keyword_matches.append(matches / len(expected))\n",
    "                results[model][\"keyword_matches\"].append(max(keyword_matches))\n",
    "                \n",
    "                # Calculate NDCG\n",
    "                if len(relevance) > 0:\n",
    "                    predicted_scores = scores[:len(relevance)]\n",
    "                    ndcg = ndcg_score([relevance], [predicted_scores])\n",
    "                    results[model][\"ndcg_scores\"].append(ndcg)\n",
    "                \n",
    "                # Print detailed results for this query\n",
    "                print(f\"\\nQuery: {query}\")\n",
    "                print(f\"Response Time: {response_time:.3f} seconds\")\n",
    "                print(f\"Top 2 Results:\")\n",
    "                for i, (doc, score) in enumerate(zip(docs[:2], scores[:2]), 1):\n",
    "                    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "                    print(f\"Preview: {doc[:150]}...\")\n",
    "                    matches = [kw for kw in expected if kw.lower() in doc.lower()]\n",
    "                    print(f\"Matching keywords: {matches}\")\n",
    "            \n",
    "            # Print summary for this model\n",
    "            print(f\"\\nSummary for {model.upper()}:\")\n",
    "            print(f\"Average Response Time: {np.mean(results[model]['response_times']):.3f} seconds\")\n",
    "            print(f\"Average Keyword Match Rate: {np.mean(results[model]['keyword_matches']):.2f}\")\n",
    "            print(f\"Average NDCG Score: {np.mean(results[model]['ndcg_scores']):.2f}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"How to implement SSR in Next.js?\",\n",
    "        \"expected_keywords\": [\n",
    "            \"server\", \"side\", \"rendering\", \"getServerSideProps\", \"SSR\",\n",
    "            \"initial props\", \"data fetching\"\n",
    "        ],\n",
    "        \"relevance_scores\": [1, 1, 1, 0, 0]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are API routes in Next.js?\",\n",
    "        \"expected_keywords\": [\n",
    "            \"api\", \"routes\", \"handler\", \"endpoint\", \"backend\",\n",
    "            \"serverless\", \"functions\"\n",
    "        ],\n",
    "        \"relevance_scores\": [1, 1, 0, 0, 0]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How to deploy Next.js?\",\n",
    "        \"expected_keywords\": [\n",
    "            \"deploy\", \"vercel\", \"production\", \"build\", \"hosting\",\n",
    "            \"environment\", \"configuration\"\n",
    "        ],\n",
    "        \"relevance_scores\": [1, 1, 1, 0, 0]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How to optimize Next.js performance?\",\n",
    "        \"expected_keywords\": [\n",
    "            \"optimize\", \"performance\", \"lazy loading\", \"images\",\n",
    "            \"caching\", \"bundling\", \"lighthouse\"\n",
    "        ],\n",
    "        \"relevance_scores\": [1, 1, 1, 0, 0]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_evaluation():\n",
    "    # Make sure retrieval system is initialized\n",
    "    if not hasattr(retrieval, 'indexes') or not retrieval.indexes:\n",
    "        print(\"Loading indexes...\")\n",
    "        retrieval.load_indexes()\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = ModelEvaluator(retrieval)\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    results = evaluator.evaluate_models(test_queries)\n",
    "    \n",
    "    # Print comparative analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Comparative Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    metrics = [\"response_times\", \"keyword_matches\", \"ndcg_scores\"]\n",
    "    metric_names = [\"Response Time (s)\", \"Keyword Match Rate\", \"NDCG Score\"]\n",
    "    \n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        print(f\"\\n{metric_name}:\")\n",
    "        for model in results:\n",
    "            avg_value = np.mean(results[model][metric])\n",
    "            print(f\"{model.upper()}: {avg_value:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation...\n",
      "\n",
      "Starting Model-by-Model Evaluation:\n",
      "\n",
      "--------------------\n",
      "Evaluating E5 Model\n",
      "--------------------\n",
      "\n",
      "Query: How to implement SSR in Next.js?\n",
      "Response Time: 0.075 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -53.3164\n",
      "Preview: Read MoreFebruary 17th, 2022+9Next.js 12.1We're excited to release one of our most requested features with Next.js 12.1: On-demand ISR (Beta) Expanded...\n",
      "Matching keywords: ['server', 'data fetching']\n",
      "\n",
      "2. Score: -53.6190\n",
      "Preview: Next.js 10 | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnB...\n",
      "Matching keywords: ['server', 'side', 'rendering', 'getServerSideProps', 'data fetching']\n",
      "\n",
      "Query: What are API routes in Next.js?\n",
      "Response Time: 0.050 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -47.5533\n",
      "Preview: Then, easily revalidate cached data and update your UI in one network roundtrip.Advanced Routing & Nested LayoutsCreate routes using the file system, ...\n",
      "Matching keywords: ['api', 'routes', 'handler', 'endpoint']\n",
      "\n",
      "2. Score: -48.2673\n",
      "Preview: Next.js by Vercel - The React FrameworkSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Sear...\n",
      "Matching keywords: ['api', 'routes', 'handler', 'endpoint']\n",
      "\n",
      "Query: How to deploy Next.js?\n",
      "Response Time: 0.053 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -52.4359\n",
      "Preview: Read MoreFebruary 17th, 2022+9Next.js 12.1We're excited to release one of our most requested features with Next.js 12.1: On-demand ISR (Beta) Expanded...\n",
      "Matching keywords: ['deploy', 'build', 'hosting', 'configuration']\n",
      "\n",
      "2. Score: -53.1065\n",
      "Preview: Then, you can use rewrites inside next.config.js to have some subpaths to be proxied to your existing app. For example, let's say you created a Next.j...\n",
      "Matching keywords: ['deploy', 'vercel', 'production']\n",
      "\n",
      "Query: How to optimize Next.js performance?\n",
      "Response Time: 0.058 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -45.0247\n",
      "Preview: Next.js 10 | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnB...\n",
      "Matching keywords: ['optimize', 'performance', 'images']\n",
      "\n",
      "2. Score: -50.2246\n",
      "Preview: We want you to know that we are not yet satisfied with the experience of using the App Router and it is our top priority moving forward. So, let's tal...\n",
      "Matching keywords: ['optimize', 'performance', 'caching', 'bundling']\n",
      "\n",
      "Summary for E5:\n",
      "Average Response Time: 0.059 seconds\n",
      "Average Keyword Match Rate: 0.64\n",
      "Average NDCG Score: 1.00\n",
      "\n",
      "--------------------\n",
      "Evaluating SBERT Model\n",
      "--------------------\n",
      "\n",
      "Query: How to implement SSR in Next.js?\n",
      "Response Time: 0.334 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -0.9060\n",
      "Preview: We appreciate early feedback from alpha testers helping us shape the future of this feature. Thank you We're thankful many of you have chosen to learn...\n",
      "Matching keywords: []\n",
      "\n",
      "2. Score: -0.9871\n",
      "Preview: Next.js 10 | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnB...\n",
      "Matching keywords: ['server', 'side', 'rendering', 'getServerSideProps', 'data fetching']\n",
      "\n",
      "Query: What are API routes in Next.js?\n",
      "Response Time: 0.017 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -0.7849\n",
      "Preview: Layouts RFC | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearn...\n",
      "Matching keywords: ['routes']\n",
      "\n",
      "2. Score: -0.8068\n",
      "Preview: Each page file exports a React Component and has an associated route based on its file name. For example: Dynamic Routes: Next.js supports Dynamic Rou...\n",
      "Matching keywords: ['routes']\n",
      "\n",
      "Query: How to deploy Next.js?\n",
      "Response Time: 0.614 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -0.9121\n",
      "Preview: Then, you can use rewrites inside next.config.js to have some subpaths to be proxied to your existing app. For example, let's say you created a Next.j...\n",
      "Matching keywords: ['deploy', 'vercel', 'production']\n",
      "\n",
      "2. Score: -0.9249\n",
      "Preview: New Next.js Documentation | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search.....\n",
      "Matching keywords: ['deploy', 'build']\n",
      "\n",
      "Query: How to optimize Next.js performance?\n",
      "Response Time: 0.622 seconds\n",
      "Top 2 Results:\n",
      "\n",
      "1. Score: -0.6871\n",
      "Preview: Next.js 10 | Next.jsSkip to contentSearch documentation...Search...⌘KShowcaseDocsBlogTemplatesEnterpriseSearch documentation...Search...⌘KDeployLearnB...\n",
      "Matching keywords: ['optimize', 'performance', 'images']\n",
      "\n",
      "2. Score: -0.7189\n",
      "Preview: We want you to know that we are not yet satisfied with the experience of using the App Router and it is our top priority moving forward. So, let's tal...\n",
      "Matching keywords: ['optimize', 'performance', 'caching', 'bundling']\n",
      "\n",
      "Summary for SBERT:\n",
      "Average Response Time: 0.397 seconds\n",
      "Average Keyword Match Rate: 0.46\n",
      "Average NDCG Score: 1.00\n",
      "\n",
      "==================================================\n",
      "Comparative Analysis\n",
      "==================================================\n",
      "\n",
      "Response Time (s):\n",
      "E5: 0.059\n",
      "SBERT: 0.397\n",
      "\n",
      "Keyword Match Rate:\n",
      "E5: 0.643\n",
      "SBERT: 0.464\n",
      "\n",
      "NDCG Score:\n",
      "E5: 1.000\n",
      "SBERT: 1.000\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = run_model_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextjs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
