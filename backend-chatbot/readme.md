# Start the server:

Run redis server:
```
docker pull redis
docker run -d --name redis-server -p 6379:6379 redis

```
Run the server:
```
python main.py
```

or 

for multi-threaded server:

```
uvicorn app.api_router:app --host 127.0.0.1 --port 8000 --workers 4 --reload
```

# Document Initalization 
```
curl -X POST "http://localhost:8000/api/v1/crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "sitemap_url": "https://nextjs.org/sitemap.xml",
       "index_name": "nextjs"
     }'
```

# Query the documentation:
```
curl -X POST "http://localhost:8000/api/v1/query" \
     -H "Content-Type: application/json" \
     -d '{
       "query": "How do I create a new Next.js project?",
       "index_name": "nextjs"
     }'

```


## Components

- Crawler Agent: User retrival service to load and crawl the sitemaps
- QA Agent: Handles user queries using the indexed documentation
- FAISS Manager: Manages vector store operations (Removed for now) since retrival service handles it
- Web Crawler: Handles sitemap parsing and content extraction (Removed for now) since retrival service handles it
- LLM Utils: Manages LLM and embedding model configurations

## Development
- To add support for a new documentation source:
    - Add the initial prompt to data/initial_prompts.json
    - Crawl and index the documentation using the /crawl endpoint
    - Start querying using the /query endpoint


##### Database Structure

For more details about the backend architecture, refer to the [Github Wiki](https://github.com/maheshj01/api-docs-ai/wiki/Backend-Architecture)

<img width="935" alt="Image" src="https://github.com/user-attachments/assets/30a1e19f-902b-4ac5-8395-b1327e9d448d" />


For Grpc generation:
```
cd backend-chatbot
python -m grpc_tools.protoc -I../backend/proto --python_out=./app/grpc --grpc_python_out=./app/grpc ../backend/proto/chat_service.proto -->

# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc

from app.grpc import chat_service_pb2 as chat__service__pb2  # Fix this import
```


## Workflow

### Redis Caching

- **Purpose**: Redis is used to cache chat history for quick retrieval, reducing the need to query the Supabase database for frequently accessed data.
- **Cache Key Structure**: The cache key for storing chat history is structured as `chat_history:{chat_id}`.
- **Cache Hit**: When a chat history request is made, the API first checks Redis for the cached data. If the data is found (cache hit), it is returned immediately.
- **Cache Miss**: If the data is not found in Redis (cache miss), the API queries Supabase to retrieve the chat history. The retrieved data is then cached in Redis for future requests.
- **Expiration**: Cached data in Redis can be set with a Time-To-Live (TTL) to ensure that stale data is not served indefinitely.

### Supabase Data Storage

- **Purpose**: Supabase is used as the primary data store for chat messages and user interactions.
- **Data Structure**: Chat messages are stored in a table called `messages`, with fields for `chat_id`, `role`, `content`, and optional metadata.
- **Fetching Data**: When a cache miss occurs, the API queries Supabase to fetch the chat history based on the provided `chat_id`.
- **Saving Data**: New chat messages are saved to Supabase when a conversation is completed, ensuring that all interactions are persisted.

### gRPC Routes

The API exposes several gRPC routes for chat interactions. Below are the key routes and their functionalities:

1. **Stream Chat Responses**
   - **Endpoint**: `/api/v1/chat/grpc/stream`
   - **Method**: `POST`
   - **Description**: Streams chat responses based on the user's query and chat history.
   - **Parameters**:
     - `chat_id`: The ID of the chat session.
     - `model_name`: The name of the model to use for generating responses.
     - `query`: The user's query.
     - `index_name`: The index name for the query.
   - **Workflow**:
     - The API checks Redis for cached chat history.
     - If found, it returns the cached history.
     - If not found, it queries Supabase for the chat history.
     - The chat history is formatted and sent to the gRPC service for response generation.
     - Responses are streamed back to the client.

2. **Get Available Models**
   - **Endpoint**: `/api/v1/grpc/models`
   - **Method**: `GET`
   - **Description**: Retrieves a list of available models from the gRPC server.
   - **Workflow**:
     - The API connects to the gRPC server and requests the list of models.
     - The response is returned to the client.

3. **Get Available Sources**
   - **Endpoint**: `/api/v1/grpc/sources`
   - **Method**: `GET`
   - **Description**: Retrieves a list of available documentation sources from the gRPC server.
   - **Workflow**:
     - Similar to the models endpoint, the API connects to the gRPC server and requests the list of sources.

4. **Get Chat History**
   - **Endpoint**: `/api/v1/chat/history/{chat_id}`
   - **Method**: `GET`
   - **Description**: Retrieves the chat history for a specific chat ID.
   - **Workflow**:
     - The API checks Redis for the cached chat history.
     - If found, it returns the cached history.
     - If not found, it queries Supabase and returns the chat history.

## Troubleshooting

### Protobuf Version Error

If you encounter the following error:

```
google.protobuf.runtime_version.VersionError: Detected incompatible Protobuf Gencode/Runtime versions when loading chat_service.proto: gencode 5.29.0 runtime 5.28.3.
```

This means there's a mismatch between the Protobuf runtime and generated code versions. To fix this:

1. Make the installation script executable:
   ```bash
   chmod +x install_protobuf.sh
   ```

2. Run the script:
   ```bash
   ./install_protobuf.sh
   ```

This will upgrade the protobuf package to the correct version.

Alternatively, you can manually install the required version:
```bash
pip install "protobuf>=5.29.0"
```
